{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as ur\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "Path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Twitter data using the MINET library\n",
    "- We use two hashtags as a query and get all associated tweets. \n",
    "- Even though we put the limit for tweets as 30k, only \\~19k and \\~6k tweets are available for the hashtags \"#tierslieux\" and \"#tierslieu\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"#tierslieux\"                                                     \n",
      "Collecting tweet:  63%|‚ñã| 19033/30000 [13:41<07:53, 23.16tweet/s, queries=1, tok                                                             \n",
      "Searching for \"#tierslieu\"                                                      \n",
      "Collecting tweet:  20%|‚ñè| 6066/30000 [04:34<18:03, 22.09tweet/s, queries=1, toke\n"
     ]
    }
   ],
   "source": [
    "!minet tw scrape tweets \"#tierslieux\" --limit 30000 > tweets_tierslieux_30000.csv\n",
    "\n",
    "!minet tw scrape tweets \"#tierslieu\" --limit 30000 > tweets_tierslieu_30000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/onkar/OneDrive/Courses/Digital_Spaces/Project(1)/tierslieux_ddps/data/tweets_tierslieu_30000.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.move(\"tweets_tierslieux_30000.csv\", Path[:-4]+'/data/tweets_tierslieux_30000.csv')\n",
    "shutil.move(\"tweets_tierslieu_30000.csv\", Path[:-4]+'/data/tweets_tierslieu_30000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>timestamp_utc</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2408979548344320</td>\n",
       "      <td>RT @fbon: La \"non biblioth√®que\" de Chris Meade...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1289409320</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4884796241088512</td>\n",
       "      <td>\"Impact des nvx modes de travail [= #teletrava...</td>\n",
       "      <td>[teletravail]</td>\n",
       "      <td>1289999600</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27334783331729408</td>\n",
       "      <td>#thirdplace #tierslieux #EnUnMot RT @hughpearm...</td>\n",
       "      <td>[enunmot, thirdplace]</td>\n",
       "      <td>1295352094</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36102389715058688</td>\n",
       "      <td>observe les amoureux qui s'b√©cotent sur les ma...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1297442455</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43616655615594496</td>\n",
       "      <td>#Tierslieu#Aubervilliers http://sebastienlucas...</td>\n",
       "      <td>[tierslieu#aubervilliers]</td>\n",
       "      <td>1299233995</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>1511736088813096969</td>\n",
       "      <td>üåê Vive la fibre num√©rique et surtout humaine üòÉ...</td>\n",
       "      <td>[coworking, hautdebit]</td>\n",
       "      <td>1649260944</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>1511744041024737283</td>\n",
       "      <td>Le Square, Jardinerie et Tiers-Lieu\\nsc√©nograp...</td>\n",
       "      <td>[jardinerie, merchandising, mobilier, sc√©nogra...</td>\n",
       "      <td>1649262840</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>1511927011471679490</td>\n",
       "      <td>üè°üíª Le PETR Pays de #Langres, regroupant 3 Comm...</td>\n",
       "      <td>[langres, teletravail]</td>\n",
       "      <td>1649306464</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>1511946875519311873</td>\n",
       "      <td>√âchanges int√©ressants lors de la conf√©rence or...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1649311200</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>1511978876452618250</td>\n",
       "      <td>üé• On parle des portes-ouvertes des Beaumonts #...</td>\n",
       "      <td>[tours]</td>\n",
       "      <td>1649318830</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24982 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0         2408979548344320  RT @fbon: La \"non biblioth√®que\" de Chris Meade...   \n",
       "1         4884796241088512  \"Impact des nvx modes de travail [= #teletrava...   \n",
       "2        27334783331729408  #thirdplace #tierslieux #EnUnMot RT @hughpearm...   \n",
       "3        36102389715058688  observe les amoureux qui s'b√©cotent sur les ma...   \n",
       "4        43616655615594496  #Tierslieu#Aubervilliers http://sebastienlucas...   \n",
       "...                    ...                                                ...   \n",
       "24977  1511736088813096969  üåê Vive la fibre num√©rique et surtout humaine üòÉ...   \n",
       "24978  1511744041024737283  Le Square, Jardinerie et Tiers-Lieu\\nsc√©nograp...   \n",
       "24979  1511927011471679490  üè°üíª Le PETR Pays de #Langres, regroupant 3 Comm...   \n",
       "24980  1511946875519311873  √âchanges int√©ressants lors de la conf√©rence or...   \n",
       "24981  1511978876452618250  üé• On parle des portes-ouvertes des Beaumonts #...   \n",
       "\n",
       "                                                hashtags timestamp_utc  year  \\\n",
       "0                                                     []    1289409320  2010   \n",
       "1                                          [teletravail]    1289999600  2010   \n",
       "2                                  [enunmot, thirdplace]    1295352094  2011   \n",
       "3                                                     []    1297442455  2011   \n",
       "4                              [tierslieu#aubervilliers]    1299233995  2011   \n",
       "...                                                  ...           ...   ...   \n",
       "24977                             [coworking, hautdebit]    1649260944  2022   \n",
       "24978  [jardinerie, merchandising, mobilier, sc√©nogra...    1649262840  2022   \n",
       "24979                             [langres, teletravail]    1649306464  2022   \n",
       "24980                                                 []    1649311200  2022   \n",
       "24981                                            [tours]    1649318830  2022   \n",
       "\n",
       "      month day  \n",
       "0        11  10  \n",
       "1        11  17  \n",
       "2         1  18  \n",
       "3         2  11  \n",
       "4         3   4  \n",
       "...     ...  ..  \n",
       "24977     4   6  \n",
       "24978     4   6  \n",
       "24979     4   7  \n",
       "24980     4   7  \n",
       "24981     4   7  \n",
       "\n",
       "[24982 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Combining the two twitter datasets\n",
    "df1 = pd.read_csv(Path[:-4]+'/data/tweets_tierslieux_30000.csv')\n",
    "df2 = pd.read_csv(Path[:-4]+'/data/tweets_tierslieu_30000.csv')\n",
    "\n",
    "\n",
    "df_combined = pd.DataFrame(columns=['id','text','hashtags','timestamp_utc','year','month','day'])\n",
    "\n",
    "for temp_df in [df1,df2]:\n",
    "    new_df = pd.DataFrame(columns=['id','text','hashtags','timestamp_utc','year','month','day'])\n",
    "    for cols in ['id','text','hashtags','timestamp_utc']:\n",
    "        new_df[cols] = temp_df[cols]\n",
    "\n",
    "    date = pd.to_datetime(temp_df['timestamp_utc'], unit='s')\n",
    "    new_df['year'] = pd.DatetimeIndex(date).year\n",
    "    new_df['month'] = pd.DatetimeIndex(date).month\n",
    "    new_df['day'] = pd.DatetimeIndex(date).day\n",
    "\n",
    "    df_combined = pd.concat([df_combined,new_df])\n",
    "\n",
    "df_combined = df_combined.drop_duplicates(ignore_index=True)\n",
    "df_combined = df_combined.sort_values(by=['timestamp_utc'], ascending=True,ignore_index=True)\n",
    "\n",
    "ori_hashtag = df_combined['hashtags']\n",
    "\n",
    "for i, hasht in enumerate(ori_hashtag):\n",
    "    temp_hasht = hasht.split('|')\n",
    "    temp_hasht = [ht for ht in temp_hasht if ht!='tierslieux']\n",
    "    temp_hasht = [ht for ht in temp_hasht if ht!='tierslieu']\n",
    "    ori_hashtag[i] = temp_hasht\n",
    "\n",
    "df_combined['hashtags'] = ori_hashtag\n",
    "df_combined.to_csv(Path[:-4]+'/data/tweets_combined.csv')\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Website data using the Beautiful soup library\n",
    "- We crawl the https://communemesure.fr/app/les-lieux website to get data for all available third places.\n",
    "- We search CSS classes with specific attributes on the html page. \n",
    "- We then go to each individual site page and collect the name, address, geographical coordinates,founding ideas and values carried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  is done\n",
      "1  is done\n",
      "2  is done\n",
      "3  is done\n",
      "4  is done\n",
      "5  is done\n",
      "6  is done\n",
      "7  is done\n",
      "8  is done\n",
      "9  is done\n",
      "10  is done\n",
      "11  is done\n",
      "12  is done\n",
      "13  is done\n",
      "14  is done\n",
      "15  is done\n",
      "16  is done\n",
      "17  is done\n",
      "18  is done\n",
      "19  is done\n",
      "20  is done\n",
      "21  is done\n",
      "22  is done\n",
      "23  is done\n",
      "24  is done\n",
      "25  is done\n",
      "26  is done\n",
      "27  is done\n",
      "28  is done\n",
      "29  is done\n",
      "30  is done\n",
      "31  is done\n",
      "32  is done\n",
      "33  is done\n",
      "34  is done\n",
      "35  is done\n",
      "36  is done\n",
      "37  is done\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Name','Address','Latitude','Longitude','Founding_idea','values carried'])\n",
    "\n",
    "url_main = ur.urlopen('https://communemesure.fr/app/les-lieux')\n",
    "soup = BeautifulSoup(url_main.read())\n",
    "\n",
    "\n",
    "main_page = soup.findAll('span', attrs={'class':'title mb-4'})\n",
    "\n",
    "for i,mp in enumerate(main_page):\n",
    "    temp_data = ['']*len(df.columns)\n",
    "    temp_data[0] = mp.text  #Name of the mesure\n",
    "\n",
    "    page_link = mp.findNext()\n",
    "\n",
    "    mesure_url = ur.urlopen(page_link['href'])\n",
    "    soup = BeautifulSoup(mesure_url.read())\n",
    "\n",
    "    ########\n",
    "    founding_values = soup.find('p',attrs={'class':'fontSize1em'})\n",
    "    data = founding_values.text\n",
    "    data = data.replace('\\'','').replace('\\r\\n','').replace('. ','.')\n",
    "    temp_data[4] = data #Founding values of the mesure\n",
    "\n",
    "    ########\n",
    "    values_carried = soup.findAll('strong',attrs={'class':'valeurs has-text-primary'})\n",
    "    val = ''\n",
    "    for vc in values_carried:\n",
    "        val += vc.text + ','\n",
    "    val = val[:-1]\n",
    "    temp_data[5] = val  #Values carried by the mesure\n",
    "\n",
    "    ########\n",
    "    location = list(soup.find('div',attrs={'class':'sous-banner sous-banner-localisation'}))\n",
    "    loc_data = location[3]\n",
    "    temp_data[1] = loc_data.text    #Address of the mesure\n",
    "\n",
    "    coord = (loc_data['href']).replace('geo:','')\n",
    "    coord = coord.split(',')\n",
    "    temp_data[2] = float(coord[0])  #Latitude\n",
    "    temp_data[3] = float(coord[1])  #Longitude\n",
    "\n",
    "    df.loc[len(df)] = temp_data\n",
    "\n",
    "    print(i, ' is done')\n",
    "\n",
    "df.to_csv(Path[:-4]+'/data/commune_mesure_website_data.csv',sep=',')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74c0b7f6186a0dc4f3622f4810ee33a8ada1128fc05dad9e06a64fd546bf2afc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
